{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "646fa0a3",
   "metadata": {},
   "source": [
    "### N·∫øu m·ªü r·ªông t·ª´ **Groom Deformer b·∫±ng ML** sang **Motion & Skin (x∆∞∆°ng + da + c∆°)** th√¨ pipeline v·∫´n gi·ªØ √Ω t∆∞·ªüng chung (pose ‚Üí deformation), nh∆∞ng ph·ª©c t·∫°p h∆°n v√¨ thay v√¨ ch·ªâ ‚Äúguides‚Äù ta ph·∫£i x·ª≠ l√Ω c·∫£ **skin mesh, muscle volume, v√† motion sequence**. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch workflow Houdini (SOP + TOP + ML nodes) \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Nguy√™n l√Ω m·ªü r·ªông t·ª´ Groom ‚Üí Motion & Skin\n",
    "\n",
    "- **Groom case**:\n",
    "  - Input: Joint pose\n",
    "  - Output: Guide deformation (n√©n PCA)\n",
    "- **Skin/Motion case**:\n",
    "  - Input: Joint pose / animation frame\n",
    "  - Output:\n",
    "    - Skin deformation (vertex delta so v·ªõi bind pose)\n",
    "    - Muscle simulation (quasi-static ground truth)\n",
    "    - Motion dynamics (velocity/acceleration fields)\n",
    "- B·∫£n ch·∫•t: v·∫´n l√† **mapping pose ‚Üí deformation**, ch·ªâ thay target t·ª´ ‚Äúhair guides‚Äù sang ‚Äúskin/muscle/mesh‚Äù.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Pipeline ML cho Motion & Skin\n",
    "\n",
    "1. **X√°c ƒë·ªãnh Input Features (Pose Parameters)**\n",
    "   - Joint rotations (Euler/quaternion)\n",
    "   - Joint positions (world/local space)\n",
    "   - C√≥ th·ªÉ th√™m: velocity, acceleration c·ªßa joint (n·∫øu mu·ªën motion-aware)\n",
    "   - Normalize:\n",
    "     - Rotation ‚Üí quaternion (4D, tr√°nh gimbal lock)\n",
    "     - Position ‚Üí relative to root (lo·∫°i b·ªè translation to√†n th√¢n)\n",
    "\n",
    "2. **Sinh Pose Dataset (Pose Randomization)**\n",
    "   - D√πng `ml pose generate SOP` nh∆∞ groom, m·ªü r·ªông gi·ªõi h·∫°n kh·ªõp d·ª±a tr√™n motion capture ho·∫∑c animation library\n",
    "   - Xu·∫•t ra nhi·ªÅu pose + motion clip sample\n",
    "\n",
    "3. **Ground-truth Deformation (Skin/Muscle Simulation)**\n",
    "   - V·ªõi m·ªói pose:\n",
    "     - D√πng Muscle & Tissue solver trong Houdini (ho·∫∑c FEM) ƒë·ªÉ t√≠nh skin deformation\n",
    "     - N·∫øu c·∫ßn motion: ch·∫°y simulation short sequence ƒë·ªÉ l·∫•y velocity field\n",
    "   - Output:\n",
    "     - Vertex positions/Œî (N√ó3)\n",
    "     - Muscle tension map\n",
    "     - Skin sliding data\n",
    "\n",
    "4. **PCA Compression (Skin/Motion Deformation)**\n",
    "   - Skin mesh c√≥ h√†ng ch·ª•c ngh√¨n vertex, c·∫ßn PCA n√©n l·∫°i:\n",
    "     - T·∫°o ma tr·∫≠n $M \\in \\mathbb{R}^{N \\times (V \\times 3)}$, v·ªõi V = s·ªë vertex\n",
    "     - D√πng PCA ‚Üí latent space (K = 64‚Äì512)\n",
    "   - Motion dynamics: c√≥ th·ªÉ d√πng PCA ri√™ng tr√™n velocity field ho·∫∑c Autoencoder 3D\n",
    "   - T√°ch PCA theo body regions (torso, arm, leg, face) gi√∫p model ch√≠nh x√°c h∆°n\n",
    "\n",
    "5. **Train ML Model (ML Train TOP)**\n",
    "   - Input: Pose features (joint quaternions + pos)\n",
    "   - Output: PCA coefficients (skin/muscle)\n",
    "   - Model:\n",
    "     - MLP: ƒë·ªß cho skin deformation c∆° b·∫£n\n",
    "     - RNN/LSTM: n·∫øu c·∫ßn motion dynamics theo chu·ªói\n",
    "   - Loss:\n",
    "     - MSE cho PCA coefficients\n",
    "     - C√≥ th·ªÉ th√™m ‚ÄúLaplacian smoothness‚Äù loss ƒë·ªÉ tr√°nh mesh rung\n",
    "\n",
    "6. **Apply Model (ML Apply SOP)**\n",
    "   - V·ªõi pose m·ªõi ho·∫∑c sequence mocap:\n",
    "     - D·ª± ƒëo√°n PCA coefficients\n",
    "     - Reconstruct deformation (basis √ó coeff)\n",
    "     - √Åp d·ª•ng v√†o skin mesh ‚Üí deform tr·ª±c ti·∫øp\n",
    "   - K·∫øt qu·∫£: Mesh c√≥ deformation g·∫ßn gi·ªëng muscle sim, nh∆∞ng realtime\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Node Setup Houdini (so v·ªõi Groom)\n",
    "\n",
    "- **Groom Deformer**\n",
    "  - `ML Example SOP` (features = joints, targets = guide deform)\n",
    "  - `ML Train TOP` (regression ‚Üí PCA coeffs)\n",
    "  - `ML Apply SOP` (pose ‚Üí guide deform)\n",
    "- **Motion & Skin Deformer**\n",
    "  - `ML Example SOP` (features = joints, targets = skin vertex Œî, muscle field)\n",
    "  - `ML Train TOP` (regression ‚Üí PCA coeffs cho skin/muscle)\n",
    "  - `ML Apply SOP` (pose ‚Üí skin deform)\n",
    "\n",
    "---\n",
    "\n",
    "## 4. ƒêi·ªÉm c·∫ßn l∆∞u √Ω khi m·ªü r·ªông\n",
    "\n",
    "1. **Dung l∆∞·ª£ng d·ªØ li·ªáu**\n",
    "   - Skin mesh >> groom guides\n",
    "   - N√™n n√©n PCA m·∫°nh (64‚Äì128 component / v√πng) ho·∫∑c d√πng Autoencoder\n",
    "\n",
    "2. **Chia v√πng c∆° th·ªÉ**\n",
    "   - X·ª≠ l√Ω mesh theo c·ª•m (torso, head, limbs)\n",
    "   - Hu·∫•n luy·ªán ML model ri√™ng cho t·ª´ng c·ª•m\n",
    "\n",
    "3. **Motion-awareness**\n",
    "   - N·∫øu c·∫ßn chuy·ªÉn ƒë·ªông m∆∞·ª£t theo th·ªùi gian: input th√™m velocity/acceleration c·ªßa joints\n",
    "   - C√≥ th·ªÉ train RNN/LSTM ƒë·ªÉ tr√°nh popping frame-to-frame\n",
    "\n",
    "4. **K·∫øt h·ª£p Groom + Skin**\n",
    "   - Sau khi c√≥ skin deformation ML ‚Üí ch·∫°y groom deformer ML\n",
    "   - Pipeline unified: Pose ‚Üí Skin + Groom deformation\n",
    "\n",
    "---\n",
    "\n",
    "## 5. ·ª®ng d·ª•ng trong c√¥ng vi·ªác c·ªßa b·∫°n\n",
    "\n",
    "- Trong game/film, thay v√¨ ch·∫°y muscle sim n·∫∑ng, b·∫°n hu·∫•n luy·ªán ML ƒë·ªÉ thay th·∫ø:\n",
    "  - Pose input t·ª´ animator ‚Üí ML d·ª± ƒëo√°n skin deform\n",
    "  - Groom deformer ML ch·∫°y song song\n",
    "- K·∫øt qu·∫£: **real-time preview**, ph√π h·ª£p pipeline Houdini ‚Üí Unreal Engine/UE5 (bake ML model th√†nh runtime graph)\n",
    "\n",
    "---\n",
    "\n",
    "**T√≥m t·∫Øt:**  \n",
    "- ML Groom Deformer l√† case study nh·ªè.  \n",
    "- M·ªü r·ªông sang Motion & Skin t·ª©c l√† thay target t·ª´ ‚Äút√≥c‚Äù sang ‚Äúmesh/c∆°/da‚Äù, gi·ªØ nguy√™n workflow:  \n",
    "  - Pose parameters ‚Üí PCA ‚Üí ML Train ‚Üí ML Apply ‚Üí Reconstruct deformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51785558",
   "metadata": {},
   "source": [
    "### Tr∆∞·ªõc khi m√¨nh gh√©p Groom v√†o, ta n√™n ph√¢n t√≠ch **Motion & Skin Deformer b·∫±ng Deep Learning (kh√¥ng ch·ªâ ML truy·ªÅn th·ªëng)** m·ªôt c√°ch chi ti·∫øt v√† chuy√™n s√¢u. ƒê√¢y s·∫Ω l√† n·ªÅn t·∫£ng ƒë·ªÉ sau ƒë√≥ b·∫°n k·∫øt h·ª£p Groom v√† c√≥ m·ªôt pipeline ‚Äúc∆° ‚Äì da ‚Äì l√¥ng‚Äù ho√†n ch·ªânh.\n",
    "\n",
    "---\n",
    "\n",
    "# 1. V·∫•n ƒë·ªÅ c∆° b·∫£n khi d·ª± ƒëo√°n **Motion & Skin**\n",
    "\n",
    "* **Input**: Pose ho·∫∑c motion (th∆∞·ªùng l√† joint rotations/quaternions, joint positions, velocity/acceleration).\n",
    "* **Output**:\n",
    "\n",
    "  * Skin deformation (mesh vertex Œî ho·∫∑c blendshape weights).\n",
    "  * Muscle field (n·∫øu m√¥ ph·ªèng c∆°).\n",
    "  * Motion dynamics (n·∫øu theo sequence).\n",
    "\n",
    "**Th√°ch th·ª©c**: Mesh c√≥ h√†ng ch·ª•c ngh√¨n vertex ‚Üí d·ªØ li·ªáu qu√° l·ªõn, kh√¥ng th·ªÉ tr·ª±c ti·∫øp train DL tr√™n raw vertices.\n",
    "\n",
    "üëâ Gi·∫£i ph√°p: **gi·∫£m chi·ªÅu d·ªØ li·ªáu b·∫±ng PCA / Autoencoder / Graph-based embedding**.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Deep Learning Pipeline cho Motion & Skin\n",
    "\n",
    "### 2.1. Bi·ªÉu di·ªÖn Input Pose\n",
    "\n",
    "* **Quaternions** cho rotation (·ªïn ƒë·ªãnh, kh√¥ng gimbal lock).\n",
    "* **Joint positions relative to root** (ƒë·∫£m b·∫£o invariant theo translation).\n",
    "* **Velocity / Acceleration** (n·∫øu mu·ªën capture motion dynamics).\n",
    "* Normalize input ƒë·ªÉ to√†n b·ªô n·∫±m trong \\[-1, 1].\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2. Bi·ªÉu di·ªÖn Output Deformation\n",
    "\n",
    "C√≥ 3 h∆∞·ªõng ch√≠nh:\n",
    "\n",
    "1. **PCA Latent Space** (truy·ªÅn th·ªëng nh∆∞ng ƒë∆°n gi·∫£n):\n",
    "\n",
    "   * N√©n mesh deformation v√†o latent vector (64‚Äì512 dims).\n",
    "   * Model DL ch·ªâ c·∫ßn d·ª± ƒëo√°n latent ‚Üí reconstruct l·∫°i mesh.\n",
    "\n",
    "2. **Autoencoder (AE / VAE)**:\n",
    "\n",
    "   * Train m·ªôt AE ri√™ng cho mesh deformation (pose-driven).\n",
    "   * Encoder n√©n deformation th√†nh latent space.\n",
    "   * Decoder reconstruct l·∫°i deformation.\n",
    "   * Sau ƒë√≥ train model ch√≠nh ƒë·ªÉ mapping: pose ‚Üí latent.\n",
    "\n",
    "3. **Graph Neural Network (GNN)** (ti√™n ti·∫øn h∆°n):\n",
    "\n",
    "   * Mesh = graph (vertex + edge).\n",
    "   * Train GNN ƒë·ªÉ d·ª± ƒëo√°n vertex deformation tr·ª±c ti·∫øp.\n",
    "   * L·ª£i: gi·ªØ topology t·ª± nhi√™n.\n",
    "   * Kh√≥: training n·∫∑ng, dataset ph·∫£i r·∫•t l·ªõn.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3. L·ª±a ch·ªçn Ki·∫øn tr√∫c Deep Learning\n",
    "\n",
    "* **MLP (baseline)**:\n",
    "\n",
    "  * Input: Pose vector (joints).\n",
    "  * Output: Latent deformation vector (PCA coeffs / AE latent).\n",
    "  * ∆Øu: D·ªÖ train, nhanh inference.\n",
    "  * Nh∆∞·ª£c: Kh√≥ capture motion temporal dynamics.\n",
    "\n",
    "* **RNN / LSTM / GRU**:\n",
    "\n",
    "  * Input: Pose sequence (frame-by-frame).\n",
    "  * Output: Sequence latent deformation.\n",
    "  * ∆Øu: Gi·ªØ continuity trong animation (kh√¥ng popping).\n",
    "  * Nh∆∞·ª£c: Kh√≥ train d√†i h·∫°n, inference k√©m real-time.\n",
    "\n",
    "* **Transformer (Temporal Attention)**:\n",
    "\n",
    "  * Input: Motion sequence (frames).\n",
    "  * Output: Sequence deformation latent.\n",
    "  * ∆Øu: Capture context xa (long-term motion).\n",
    "  * Nh∆∞·ª£c: Training t·ªën GPU, c·∫ßn nhi·ªÅu data.\n",
    "\n",
    "* **Graph Convolutional Network (GCN/GNN)**:\n",
    "\n",
    "  * Input: Joint graph / Mesh graph.\n",
    "  * Output: Vertex displacement.\n",
    "  * ∆Øu: Gi·ªØ topology x∆∞∆°ng + mesh t·ª± nhi√™n.\n",
    "  * Nh∆∞·ª£c: Ph·ª©c t·∫°p, kh√≥ tri·ªÉn khai real-time trong game.\n",
    "\n",
    "üëâ V·ªõi film/game, pipeline hay d√πng l√† **Pose (quaternion) ‚Üí MLP ‚Üí PCA latent ‚Üí Mesh deformation**. N·∫øu m·ªü r·ªông cho motion li√™n t·ª•c th√¨ th√™m **RNN ho·∫∑c Transformer** v√†o MLP.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Quy tr√¨nh Training chuy√™n s√¢u\n",
    "\n",
    "### 3.1. Data Preparation\n",
    "\n",
    "* Sinh **pose randomization** (t·ª´ motion capture ho·∫∑c joint limits).\n",
    "* Ch·∫°y muscle/skin simulation ƒë·ªÉ l·∫•y ground-truth mesh deform.\n",
    "* N√©n b·∫±ng PCA/Autoencoder.\n",
    "\n",
    "### 3.2. Training Loss\n",
    "\n",
    "* **Reconstruction Loss (MSE)**: so s√°nh mesh predicted vs mesh sim.\n",
    "* **Smoothness Loss**: penalize khi mesh jitter frame-to-frame.\n",
    "* **Regularization Loss**: tr√°nh overfit.\n",
    "* **Physics-informed Loss (t√πy ch·ªçn)**: gi·ªØ volume, collision penalty.\n",
    "\n",
    "### 3.3. Evaluation Metrics\n",
    "\n",
    "* Vertex-to-vertex L2 distance.\n",
    "* Normal consistency (surface smoothness).\n",
    "* Temporal coherence (mesh kh√¥ng rung).\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Workflow Node trong Houdini (Deep Learning oriented)\n",
    "\n",
    "### 4.1. Data Creation\n",
    "\n",
    "* SOP: `ml pose generate` ‚Üí sinh pose dataset.\n",
    "* TOP: ch·∫°y sim skin/muscle cho m·ªói pose.\n",
    "* SOP: xu·∫•t deformation (delta).\n",
    "\n",
    "### 4.2. Latent Compression\n",
    "\n",
    "* SOP: PCA (truy·ªÅn th·ªëng) ho·∫∑c Python Node ch·∫°y Autoencoder offline.\n",
    "* Xu·∫•t latent dataset.\n",
    "\n",
    "### 4.3. Training\n",
    "\n",
    "* TOP: `ML Train`\n",
    "\n",
    "  * Input: joint pose features.\n",
    "  * Target: PCA latent / AE latent.\n",
    "  * Model: ch·ªçn MLP ho·∫∑c MLP+RNN.\n",
    "\n",
    "### 4.4. Inference\n",
    "\n",
    "* SOP: `ML Apply`\n",
    "\n",
    "  * Input: new pose (ho·∫∑c sequence).\n",
    "  * Output: latent deformation ‚Üí reconstruct mesh skin.\n",
    "\n",
    "---\n",
    "\n",
    "# 5. H∆∞·ªõng t·ªëi ∆∞u cho **Motion & Skin b·∫±ng DL**\n",
    "\n",
    "1. **T√°ch v√πng c∆° th·ªÉ**: train ri√™ng torso, head, limbs ‚Üí tƒÉng accuracy.\n",
    "2. **Temporal model**: d√πng RNN/LSTM n·∫øu motion continuity quan tr·ªçng.\n",
    "3. **Hybrid PCA + Autoencoder**: PCA gi·∫£m chi·ªÅu tr∆∞·ªõc, AE refine ƒë·ªÉ gi·ªØ detail.\n",
    "4. **Export Runtime**: model sau training c√≥ th·ªÉ convert th√†nh ONNX/TensorRT ƒë·ªÉ ch·∫°y real-time trong UE5.\n",
    "\n",
    "---\n",
    "\n",
    "# 6. So s√°nh: Groom vs Motion & Skin\n",
    "\n",
    "| Groom ML Deformer                         | Motion & Skin DL Deformer                                          |\n",
    "| ----------------------------------------- | ------------------------------------------------------------------ |\n",
    "| Input = pose ‚Üí Output = guide deformation | Input = pose (ho·∫∑c motion) ‚Üí Output = skin/muscle mesh deformation |\n",
    "| PCA 64‚Äì128 comp. ƒë·ªß                       | PCA/AE c·∫ßn 128‚Äì512 comp. cho skin                                  |\n",
    "| MLP th∆∞·ªùng l√† ƒë·ªß                          | C√≥ th·ªÉ c·∫ßn RNN/Transformer cho motion continuity                   |\n",
    "| Training nhanh, dataset nh·ªè               | Dataset l·ªõn (m√¥ ph·ªèng skin), training n·∫∑ng h∆°n                     |\n",
    "| D√πng ƒë·ªÉ layer hair/fur ƒë·∫πp h∆°n            | D√πng ƒë·ªÉ thay th·∫ø muscle sim / skin sliding                         |\n",
    "\n",
    "---\n",
    "\n",
    "üëâ T√≥m t·∫Øt g·∫Øn g·ªçn:\n",
    "\n",
    "* V·ªõi **Motion & Skin**, deep learning n√™n ƒëi theo h∆∞·ªõng **Pose ‚Üí PCA/AE latent ‚Üí MLP/RNN ‚Üí Mesh Deform**.\n",
    "* Groom th√¨ ch·ªâ c·∫ßn **MLP + PCA** l√† ƒë·ªß.\n",
    "* Khi k·∫øt h·ª£p: b·∫°n s·∫Ω c√≥ pipeline **Pose ‚Üí Skin Deform ‚Üí Groom Deform**, c·∫£ hai ƒë·ªÅu ch·∫°y ML inference real-time thay v√¨ simulation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6df5b3",
   "metadata": {},
   "source": [
    "### Tuy·ªát ‚Äî d∆∞·ªõi ƒë√¢y l√† ‚Äúb·∫£n thi·∫øt k·∫ø‚Äù chi ti·∫øt cho **workflow Deep Learning: Motion & Skin trong Houdini** v√† c√°c **c√°ch export sang Unreal Engine (UE)**. M√¨nh chia th√†nh: (A) pipeline s·∫£n xu·∫•t d·ªØ li·ªáu + hu·∫•n luy·ªán trong Houdini, (B) suy lu·∫≠n & ki·ªÉm th·ª≠ trong Houdini, (C) c√°c con ƒë∆∞·ªùng ƒë∆∞a sang UE (t√πy y√™u c·∫ßu realtime/ƒë·ªô linh ho·∫°t), k√®m checklist th·ª±c thi.\n",
    "\n",
    "---\n",
    "\n",
    "# A) Houdini: Pipeline t·∫°o d·ªØ li·ªáu & hu·∫•n luy·ªán DL cho Motion & Skin\n",
    "\n",
    "```\n",
    "[Anim Rig / Rest Rig]\n",
    "        ‚îÇ\n",
    "        ‚îú‚îÄ‚ñ∫ (1) Pose Generation (SOP)  ‚Üí  t·∫≠p pose / motion clip\n",
    "        ‚îÇ         ‚Ä¢ ml pose generate SOP\n",
    "        ‚îÇ         ‚Ä¢ Gi·ªõi h·∫°n kh·ªõp / sampling theo Gaussian\n",
    "        ‚îÇ\n",
    "        ‚îú‚îÄ‚ñ∫ (2) GT Deformation (TOPs)  ‚Üí  ground-truth skin/muscle\n",
    "        ‚îÇ         ‚Ä¢ Muscle/Tissue/FEM (quasi-static)\n",
    "        ‚îÇ         ‚Ä¢ Xu·∫•t per-vertex Œî (so v·ªõi bind), normal/tangent (tu·ª≥)\n",
    "        ‚îÇ\n",
    "        ‚îú‚îÄ‚ñ∫ (3) Feature Builder (SOP) ‚Üí  vector ho√° input\n",
    "        ‚îÇ         ‚Ä¢ Joint quaternions (khuy·∫øn ngh·ªã), pos rel-root\n",
    "        ‚îÇ         ‚Ä¢ (tu·ª≥) velocity/accel cho motion-aware\n",
    "        ‚îÇ         ‚Ä¢ Normalize & pack th√†nh attributes\n",
    "        ‚îÇ\n",
    "        ‚îú‚îÄ‚ñ∫ (4) Latent Compression     ‚Üí  gi·∫£m chi·ªÅu output\n",
    "        ‚îÇ         ‚Ä¢ PCA (64‚Äì512)   ho·∫∑c   Autoencoder (AE/VAE)\n",
    "        ‚îÇ         ‚Ä¢ T√°ch v√πng (torso/head/arm/leg) ƒë·ªÉ tƒÉng fidelity\n",
    "        ‚îÇ\n",
    "        ‚îî‚îÄ‚ñ∫ (5) ML/DL Train (TOPs)     ‚Üí  h·ªçc pose‚Üílatent\n",
    "                  ‚Ä¢ MLP (baseline), +RNN/Transformer n·∫øu c·∫ßn temporal\n",
    "                  ‚Ä¢ Loss: MSE(recon) + temporal smooth + (tu·ª≥) Laplacian\n",
    "                  ‚Ä¢ Xu·∫•t model + stats (mean/std), PCA basis/AE decoder\n",
    "```\n",
    "\n",
    "### (1) Pose Generation\n",
    "\n",
    "* `ml pose generate SOP`: nh·∫≠p skeleton, ƒë·∫∑t joint limits, s·ªë l∆∞·ª£ng pose, seed; n·∫øu c√≥ motion libraries ‚Üí xen k·∫Ω ‚Äúrandom pose‚Äù v√† ‚Äúpose t·ª´ clip‚Äù.\n",
    "* L∆∞u c√πng **joint name map** (ƒë·ªÉ kh·ªõp v·ªõi UE sau n√†y).\n",
    "\n",
    "### (2) Ground-truth Deformation (GT)\n",
    "\n",
    "* TOP graph: m·ªói pose ch·∫°y muscle/skin sim ‚Üí xu·∫•t **per-vertex delta** (`@dP`) so v·ªõi bind.\n",
    "* L∆∞u th√™m **mask/region id** cho chia v√πng PCA/AE.\n",
    "\n",
    "### (3) Feature Builder\n",
    "\n",
    "* T·∫°o feature vector ·ªïn ƒë·ªãnh:\n",
    "\n",
    "  * Rotation ‚Üí **quaternion (x,y,z,w)** t·ª´ng joint, theo local ho·∫∑c component space th·ªëng nh·∫•t.\n",
    "  * Position ‚Üí **relative-to-root**, ƒë√£ scale-normalize (chi·ªÅu cao nh√¢n v·∫≠t).\n",
    "  * (Tu·ª≥) **dPose/dt** n·∫øu b√†i to√°n temporal.\n",
    "* Chu·∫©n ho√° v√†o \\[-1,1] ho·∫∑c z-score.\n",
    "\n",
    "### (4) Latent Compression\n",
    "\n",
    "* **PCA**:\n",
    "\n",
    "  * Per-region PCA (vd: torso=128, arms=64/arm, legs=64/leg, head=128).\n",
    "  * L∆∞u: `basis_[region].npy`, `mean_[region].npy`, `scale.json`.\n",
    "* **AE/VAE** (n·∫øu mu·ªën gi·ªØ phi tuy·∫øn t·ªët h∆°n):\n",
    "\n",
    "  * Train AE ri√™ng trong TOPs (Python job) ‚Üí `decoder_[region].onnx` + `encoder` (d√πng trong training; runtime ch·ªâ c·∫ßn decoder n·∫øu model ch√≠nh xu·∫•t latent).\n",
    "\n",
    "### (5) ML/DL Train\n",
    "\n",
    "* **Input**: pose features (F).\n",
    "* **Target**: latent (K) c·ªßa t·ª´ng region (gh√©p l·∫°i ho·∫∑c train multi-head).\n",
    "* **Model**:\n",
    "\n",
    "  * Real-time: **MLP** (2‚Äì4 hidden layers, 128‚Äì512 units).\n",
    "  * Temporal: **MLP + GRU/LSTM** ho·∫∑c **Temporal Transformer** (window 8‚Äì32 frames).\n",
    "* **Loss**:\n",
    "\n",
    "  * `L = MSE(latent_pred, latent_gt)`\n",
    "  * * (tu·ª≥) smoothness qua th·ªùi gian (TV loss)\n",
    "  * * (tu·ª≥) Laplacian reg tr√™n mesh khi reconstruct ƒë·ªÉ h·∫°n ch·∫ø jitter.\n",
    "* **Artifacts c·∫ßn xu·∫•t**:\n",
    "\n",
    "  * `pose2latent.onnx` (ho·∫∑c `.mlmodel` n·∫øu d√πng ML nodes c·ªßa Houdini).\n",
    "  * `pca_basis_*.npy` + `pca_mean_*.npy` **ho·∫∑c** `decoder_*.onnx` (AE).\n",
    "  * `feature_stats.json` (mean/std c·ªßa features, joint order).\n",
    "  * `skeleton_map.json` (t√™n x∆∞∆°ng, th·ª© t·ª±, kh√¥ng gian t·ªça ƒë·ªô).\n",
    "\n",
    "---\n",
    "\n",
    "# B) Houdini: Suy lu·∫≠n + ki·ªÉm th·ª≠ n·ªôi b·ªô\n",
    "\n",
    "```\n",
    "[Pose (SOP)] ‚Üí Normalize ‚Üí pose2latent (ONNX/ML node)\n",
    "      ‚Üí (PCA) latent ‚Üí basis√ócoeff + mean ‚Üí per-vertex Œî   \\\n",
    "      ‚Üí (AE)  latent ‚Üí decoder(ONNX)      ‚Üí per-vertex Œî    } ‚Üí Apply to mesh ‚Üí Render/QA\n",
    "```\n",
    "\n",
    "* **ML Apply SOP** (n·∫øu d√πng ML nodes Houdini) *ho·∫∑c* Python SOP g·ªçi ONNX Runtime ƒë·ªÉ ch·∫°y:\n",
    "\n",
    "  * L·∫•y pose hi·ªán t·∫°i ‚Üí chu·∫©n ho√° ‚Üí **pose2latent**.\n",
    "  * Reconstruct:\n",
    "\n",
    "    * PCA: `dP = B¬∑c + Œº` (per-region r·ªìi gh√©p).\n",
    "    * AE: `dP = Decoder(c)`.\n",
    "  * √Åp v√†o mesh ‚Üí so s√°nh GT vs Pred (L2, angle normal, temporal jitter).\n",
    "* **QA nhanh**:\n",
    "\n",
    "  * Heatmap `|dP_pred - dP_gt|`, ƒë·ªì th·ªã loss theo frame.\n",
    "  * Ki·ªÉm tra c√°c pose bi√™n (extreme) c√≥ ‚Äún·ªï l∆∞·ªõi‚Äù/self-intersection kh√¥ng.\n",
    "\n",
    "---\n",
    "\n",
    "# C) Export sang Unreal Engine (nhi·ªÅu con ƒë∆∞·ªùng, ch·ªçn theo nhu c·∫ßu)\n",
    "\n",
    "## C.1) **PCA Morph Targets + Runtime Coefficients (khuy·∫øn ngh·ªã, g·ªçn ‚Äì realtime t·ªët)**\n",
    "\n",
    "**√ù t∆∞·ªüng**: Export PCA basis th√†nh **morph targets**; trong UE, m·ªôt model nh·ªè **d·ª± ƒëo√°n K h·ªá s·ªë** r·ªìi **ƒëi·ªÅu khi·ªÉn morphs**.\n",
    "\n",
    "**Chu·ªói th·ª±c thi**\n",
    "\n",
    "1. **Houdini ‚Üí FBX Skeletal Mesh**: bind mesh + skeleton (tr√πng t√™n joints v·ªõi UE).\n",
    "2. **Houdini ‚Üí Morph Targets**: v·ªõi m·ªói **PCA component** c·ªßa t·ª´ng region, bake th√†nh 1 morph target (ƒë·∫∑t t√™n quy ∆∞·ªõc):\n",
    "\n",
    "   * `MT_Torso_PC_000 ... MT_Torso_PC_127`\n",
    "   * `MT_ArmL_PC_000 ...`\n",
    "   * (Gi·ªõi h·∫°n morph UE \\~th·ª±c t·∫ø v√†i trƒÉm; chia region ƒë·ªÉ ki·ªÉm so√°t s·ªë l∆∞·ª£ng)\n",
    "3. **Houdini ‚Üí JSON stats**:\n",
    "\n",
    "   * `feature_stats.json`, `skeleton_map.json`, `pca_mean_*.npy` (kh√¥ng c·∫ßn basis ·ªü UE v√¨ basis ƒë√£ ‚Äúh√≥a th√¢n‚Äù th√†nh morphs).\n",
    "4. **Model pose‚Üícoeff**:\n",
    "\n",
    "   * Xu·∫•t `pose2latent.onnx`.\n",
    "5. **UE Runtime**:\n",
    "\n",
    "   * **ƒê·ªçc pose x∆∞∆°ng** trong Anim Graph (Control Rig ho·∫∑c tr·ª±c ti·∫øp t·ª´ Pose Link).\n",
    "   * Chu·∫©n ho√° theo `feature_stats`.\n",
    "   * **ONNX Runtime** (plugin) ch·∫°y `pose2latent.onnx` ‚Üí nh·∫≠n vector h·ªá s·ªë K cho t·ª´ng region.\n",
    "   * **Set Morph Target Weights** = h·ªá s·ªë (c√≥ th·ªÉ scale/offset).\n",
    "   * (Tu·ª≥) Low-pass filter nh·ªè ƒë·ªÉ m∆∞·ª£t.\n",
    "\n",
    "**∆Øu**:\n",
    "\n",
    "* Kh√¥ng ph·∫£i update to√†n b·ªô vertices b·∫±ng compute shader ‚Üí r·∫ª.\n",
    "* ƒÇn kh·ªõp pipeline anim/morph c√≥ s·∫µn c·ªßa UE.\n",
    "  **Nh∆∞·ª£c**:\n",
    "* S·ªë morph target nhi·ªÅu ‚Üí tƒÉng memory & k√≠ch th∆∞·ªõc asset.\n",
    "* C·∫ßn k·ª∑ lu·∫≠t ƒë·∫∑t t√™n/qu·∫£n l√Ω region.\n",
    "\n",
    "---\n",
    "\n",
    "## C.2) **UE ML Deformer (Plugin)**\n",
    "\n",
    "**√ù t∆∞·ªüng**: D√πng h·∫° t·∫ßng ML Deformer c·ªßa UE ƒë·ªÉ train/infer ngay trong UE, nh∆∞ng **dataset** ƒë∆∞·ª£c ‚Äún·∫•u‚Äù t·ª´ Houdini.\n",
    "\n",
    "**Chu·ªói th·ª±c thi (kh√°i ni·ªám)**\n",
    "\n",
    "1. T·ª´ Houdini export **neutral mesh**, **skeletal anim**, v√† **target deltas** theo t·ª´ng pose/frame (c√≥ th·ªÉ ·ªü d·∫°ng morph frame ho·∫∑c point cache).\n",
    "2. Trong UE ML Deformer, t·∫°o asset **Training Data** b·∫±ng d·ªØ li·ªáu n√†y (pose ‚Üí target delta).\n",
    "3. Train trong UE ƒë·ªÉ c√≥ **runtime deformer** (NN + graph UE).\n",
    "\n",
    "**∆Øu**:\n",
    "\n",
    "* Tr·ªçn h·ªá sinh th√°i UE, tr√¨nh b√†y t·ªët, √≠t custom code.\n",
    "  **Nh∆∞·ª£c**:\n",
    "* Ph·ª• thu·ªôc phi√™n b·∫£n UE & plugin; data ingestion c·∫ßn ƒë√∫ng ƒë·ªãnh d·∫°ng; √≠t ki·ªÉm so√°t h∆°n so v·ªõi PCA-morph/ONNX t·ª± do.\n",
    "\n",
    "---\n",
    "\n",
    "## C.3) **ONNX Runtime + Compute Shader (vertex Œî tr·ª±c ti·∫øp)**\n",
    "\n",
    "**√ù t∆∞·ªüng**: Kh√¥ng d√πng morphs. Model xu·∫•t **per-vertex delta** (K l·ªõn). D√πng **Compute Shader** c·∫≠p nh·∫≠t buffer vertex m·ªói frame.\n",
    "\n",
    "**Chu·ªói th·ª±c thi**\n",
    "\n",
    "1. Export `pose2latent.onnx` v√† **`decoder.onnx`** (AE) ho·∫∑c `pca_basis_*.npy` n·∫øu v·∫´n PCA (khi ƒë√≥ compute shader ch·ªâ l√†m `B¬∑c+Œº`).\n",
    "2. UE C++ plugin:\n",
    "\n",
    "   * L·∫•y x∆∞∆°ng ‚Üí build feature ‚Üí ONNX infer (pose‚Üílatent).\n",
    "   * **AE**: ONNX infer ti·∫øp (latent‚ÜíŒî) **ho·∫∑c**\n",
    "     **PCA**: compute shader nh√¢n ma tr·∫≠n `B¬∑c` tr√™n GPU (r·∫•t nhanh) + c·ªông mean.\n",
    "   * Ghi Œî v√†o **deform buffer** r·ªìi √°p l√™n mesh.\n",
    "\n",
    "**∆Øu**:\n",
    "\n",
    "* T·ª± do nh·∫•t, kh√¥ng gi·ªõi h·∫°n morph.\n",
    "  **Nh∆∞·ª£c**:\n",
    "* C·∫ßn k·ªπ nƒÉng C++/RHI/compute shader; qu·∫£n l√Ω ƒë·ªìng b·ªô buffer, skin pass.\n",
    "\n",
    "---\n",
    "\n",
    "## C.4) **Vertex Animation Texture (VAT) / Point Cache (baked)**\n",
    "\n",
    "* D√πng cho **phi t∆∞∆°ng t√°c** (cinematic/previz).\n",
    "* Xu·∫•t d√£y Œî theo frame ‚Üí bake v√†o texture ‚Üí UE ƒë·ªçc texture animate.\n",
    "* Kh√¥ng ph·∫£n h·ªìi pose realtime ‚Üí ph√π h·ª£p ‚Äúshot-based‚Äù.\n",
    "\n",
    "---\n",
    "\n",
    "# D) Chu·∫©n ho√° d·ªØ li·ªáu & ƒë·∫∑t t√™n (ƒë·ªÉ 2 chi·ªÅu HDA ‚Üî UE ‚ÄúƒÉn r∆°‚Äù)\n",
    "\n",
    "* **Skeleton**: joint names & hierarchy **y h·ªát** gi·ªØa Houdini v√† UE.\n",
    "* **Kh√¥ng gian t·ªça ƒë·ªô**: ch·ªçn 1 ki·ªÉu (component/local), th·ªëng nh·∫•t trong `skeleton_map.json`.\n",
    "* **Feature Stats**: `mean/std` ƒë·ªÉ normalize/inverse normalize trong UE (ƒë·∫£m b·∫£o ƒë·∫ßu v√†o model ƒë√∫ng scale).\n",
    "* **Morph Targets** (C.1):\n",
    "\n",
    "  * Quy ∆∞·ªõc: `MT_<Region>_PC_<idx>`; l∆∞u k√®m `region_ranges.json` ƒë·ªÉ bi·∫øt mapping morph ‚Üî latent index.\n",
    "* **PCA/AE Artifacts**:\n",
    "\n",
    "  * PCA: `pca_mean_*.npy`, (n·∫øu kh√¥ng bake morph) `pca_basis_*.npy`.\n",
    "  * AE: `decoder_*.onnx` (runtime), `encoder` ch·ªâ d√πng train.\n",
    "\n",
    "---\n",
    "\n",
    "# E) Quy tr√¨nh ki·ªÉm th·ª≠ khi sang UE\n",
    "\n",
    "1. **Unit test** v√†i pose chu·∫©n (A-pose, T-pose, extreme bends): so s√°nh mesh UE vs Houdini (L2/vertex, angle normal).\n",
    "2. **Stress test** motion nhanh: ki·ªÉm tra jitter, ‚Äún·ªï l∆∞·ªõi‚Äù, tr·ªÖ khung.\n",
    "3. **Hi·ªáu nƒÉng**:\n",
    "\n",
    "   * Morph route: ƒëo CPU cost ƒë·∫∑t morphs + skinned mesh pass.\n",
    "   * Compute route: ƒëo GPU time c·ªßa compute + skin.\n",
    "4. **Fallback**: n·∫øu model kh√¥ng t·∫£i ƒë∆∞·ª£c ‚Üí r∆°i v·ªÅ skinning truy·ªÅn th·ªëng.\n",
    "\n",
    "---\n",
    "\n",
    "# F) L·ªô tr√¨nh g·ª£i √Ω (th·ª±c d·ª•ng, d·ªÖ tri·ªÉn khai)\n",
    "\n",
    "* **Giai ƒëo·∫°n 1 (baseline)**: PCA theo v√πng + **MLP** ‚Üí **C.1 (Morph + ONNX)**. Nhanh, ƒë·∫πp, d·ªÖ debug.\n",
    "* **Giai ƒëo·∫°n 2 (n√¢ng c·∫•p temporal)**: th√™m **GRU** ho·∫∑c Temporal Transformer (c·ª≠a s·ªë 16‚Äì32 frames), v·∫´n gi·ªØ PCA-morph.\n",
    "* **Giai ƒëo·∫°n 3 (t·ªëi ƒëa h√≥a chi ti·∫øt)**: thay PCA b·∫±ng **AE decoder** + compute shader (C.3) ƒë·ªÉ v∆∞·ª£t gi·ªõi h·∫°n morph; ho·∫∑c chuy·ªÉn h·∫≥n sang **UE ML Deformer** n·∫øu pipeline c·ªßa team thi√™n UE.\n",
    "\n",
    "---\n",
    "\n",
    "## M·∫´u pseudo-export (tham kh·∫£o t√™n file)\n",
    "\n",
    "```\n",
    "/export/\n",
    "  skeletal_mesh.fbx\n",
    "  feature_stats.json         # mean/std, order joints, spaces\n",
    "  skeleton_map.json          # name remap HDA‚ÜîUE\n",
    "  regions.json               # torso/arms/legs/head vertex groups\n",
    "  # PCA route:\n",
    "  pca_mean_torso.npy\n",
    "  pca_basis_torso.npy        # (n·∫øu kh√¥ng bake th√†nh morph)\n",
    "  ...\n",
    "  # AE route:\n",
    "  decoder_torso.onnx\n",
    "  decoder_armL.onnx\n",
    "  ...\n",
    "  pose2latent.onnx\n",
    "```\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b3a95d",
   "metadata": {},
   "source": [
    "### Tuy·ªát ‚Äî m√¨nh s·∫Ω l√†m hai ph·∫ßn:\n",
    "\n",
    "1. **Checklist Houdini (SOP/TOP) cho pipeline PCA Morph + ONNX**\n",
    "2. **S∆° ƒë·ªì Anim Blueprint trong UE ƒë·ªÉ ch·∫°y inference + apply morph target**\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Checklist Houdini ‚Äì PCA Morph + ONNX\n",
    "\n",
    "### (A) Chu·∫©n b·ªã d·ªØ li·ªáu\n",
    "\n",
    "* **SOP Level**\n",
    "\n",
    "  * Import Rigged Mesh (FBX/Character HDA).\n",
    "  * `ml pose generate SOP` ‚Üí t·∫°o b·ªô pose.\n",
    "  * L∆∞u `skeleton_map.json` (order joint, joint name map UE).\n",
    "\n",
    "* **TOP Level**\n",
    "\n",
    "  * `ROP Fetch` ‚Üí ch·∫°y muscle/skin sim cho t·ª´ng pose.\n",
    "  * Xu·∫•t **per-vertex Œî** (so v·ªõi bind pose).\n",
    "\n",
    "---\n",
    "\n",
    "### (B) Feature Vector (Input)\n",
    "\n",
    "* **Python SOP / Wrangle SOP**:\n",
    "\n",
    "  * Extract joint rotations ‚Üí quaternion.\n",
    "  * Normalize root transform.\n",
    "  * Xu·∫•t `feature.npy`.\n",
    "* Ghi `feature_stats.json` (mean/std).\n",
    "\n",
    "---\n",
    "\n",
    "### (C) Latent Compression (Output)\n",
    "\n",
    "* **Python Processor (TOP)**:\n",
    "\n",
    "  * Run PCA per region (torso/arms/legs/head).\n",
    "  * Save:\n",
    "\n",
    "    * `pca_mean_region.npy`\n",
    "    * `pca_basis_region.npy`\n",
    "  * Rebuild Œî = basis¬∑coeff + mean ‚Üí QA.\n",
    "\n",
    "* **Morph Target Baking (SOP)**:\n",
    "\n",
    "  * V·ªõi m·ªói PCA component ‚Üí t·∫°o mesh morphed.\n",
    "  * Export ra **FBX Morph Targets** (naming convention: `MT_<Region>_PC_<Idx>`).\n",
    "\n",
    "---\n",
    "\n",
    "### (D) ML Training\n",
    "\n",
    "* **ML Train TOP**:\n",
    "\n",
    "  * Input = `features.npy`, Target = PCA coeffs.\n",
    "  * Model = MLP (2‚Äì3 hidden layers, 256‚Äì512 units).\n",
    "  * Loss = MSE.\n",
    "  * Output = `pose2latent.onnx`.\n",
    "\n",
    "---\n",
    "\n",
    "### (E) Xu·∫•t package cho UE\n",
    "\n",
    "```\n",
    "/export/\n",
    "  skeletal_mesh_with_morphs.fbx\n",
    "  pose2latent.onnx\n",
    "  feature_stats.json\n",
    "  skeleton_map.json\n",
    "  region_ranges.json   # mapping latent index -> morph target\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 2. UE ‚Äì Anim Blueprint (PCA Morph + ONNX)\n",
    "\n",
    "### **Anim Graph Flow**\n",
    "\n",
    "```\n",
    "[Input Pose from Control Rig / AnimBP]  \n",
    "        ‚îÇ\n",
    "        ‚ñº\n",
    "(Node) Extract Local Rotations\n",
    "        ‚îÇ\n",
    "        ‚ñº\n",
    "Normalize (use feature_stats.json)\n",
    "        ‚îÇ\n",
    "        ‚ñº\n",
    "ONNX Inference (pose2latent.onnx) ‚Üí Latent Vector (coeffs)\n",
    "        ‚îÇ\n",
    "        ‚îú‚îÄ‚ñ∫ Split by Region\n",
    "        ‚îÇ       ‚îú‚îÄ torso coeffs ‚Üí set morph targets MT_Torso_PC_000...  \n",
    "        ‚îÇ       ‚îú‚îÄ armL coeffs ‚Üí MT_ArmL_PC_000...  \n",
    "        ‚îÇ       ‚îî‚îÄ etc.\n",
    "        ‚îÇ\n",
    "        ‚ñº\n",
    "Apply Morph Targets ‚Üí Mesh\n",
    "```\n",
    "\n",
    "### **Implementation Notes**\n",
    "\n",
    "* **ONNX Runtime plugin** (Epic/Marketplace).\n",
    "* Blueprint function:\n",
    "\n",
    "  * `GetBoneTransform` (for skeleton input).\n",
    "  * Custom `NormalizeFeatures` (C++/BP).\n",
    "  * `RunONNXModel` ‚Üí outputs array coeffs.\n",
    "  * `SetMorphTarget` in loop.\n",
    "* (Tu·ª≥ ch·ªçn) Filter coeffs (Exponential Smoothing) ƒë·ªÉ m∆∞·ª£t.\n",
    "\n",
    "---\n",
    "\n",
    "üëâ Nh∆∞ v·∫≠y b·∫°n c√≥ m·ªôt pipeline kh√©p k√≠n:\n",
    "\n",
    "* Houdini train + PCA ‚Üí morph targets.\n",
    "* Xu·∫•t ONNX model d·ª± ƒëo√°n coeff.\n",
    "* UE AnimBP apply coeff v√†o morph.\n",
    "\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fc2644",
   "metadata": {},
   "source": [
    "### T·ªët üëç, m√¨nh s·∫Ω gi·∫£i th√≠ch chi ti·∫øt t·ª´ng kh·ªëi trong s∆° ƒë·ªì:\n",
    "\n",
    "---\n",
    "\n",
    "# üü¶ Houdini (Data Prep + Training)\n",
    "\n",
    "### **Rigged Mesh (FBX)**\n",
    "\n",
    "* Nh·∫≠p nh√¢n v·∫≠t ƒë√£ rig t·ª´ Maya/Blender/UE.\n",
    "* ƒê√¢y l√† mesh + skeleton chu·∫©n ƒë·ªÉ l√†m ground-truth.\n",
    "\n",
    "### **Pose Generation (ml pose generate SOP)**\n",
    "\n",
    "* Sinh t·∫≠p pose ƒëa d·∫°ng (rotation c√°c joints).\n",
    "* ƒê·∫£m b·∫£o bao ph·ªß motion space ‚Üí tr√°nh overfit.\n",
    "\n",
    "### **Muscle/Skin Simulation (TOPs)**\n",
    "\n",
    "* Ch·∫°y muscle/skin sim (FEM ho·∫∑c Vellum).\n",
    "* K·∫øt qu·∫£: mesh deform ch√≠nh x√°c theo pose.\n",
    "\n",
    "### **Œî Vertex Cache**\n",
    "\n",
    "* T√≠nh delta mesh (so v·ªõi bind pose).\n",
    "* ƒê√¢y l√† target ground-truth ƒë·ªÉ train ML.\n",
    "\n",
    "### **Feature Extract (Joint Rotations)**\n",
    "\n",
    "* L·∫•y quaternion/rotation matrix cho m·ªói joint.\n",
    "* Normalize root ƒë·ªÉ pose invariant.\n",
    "* Xu·∫•t th√†nh feature vector.\n",
    "\n",
    "### **PCA Compression (per Region)**\n",
    "\n",
    "* N√©n delta mesh th√†nh latent coeffs b·∫±ng PCA.\n",
    "* Chia theo region (torso, head, arms, legs).\n",
    "* Gi·∫£m t·ª´ h√†ng ngh√¨n vertex ‚Üí v√†i ch·ª•c coeff.\n",
    "\n",
    "### **Morph Target Baking (FBX Export)**\n",
    "\n",
    "* V·ªõi m·ªói PCA component ‚Üí t·∫°o m·ªôt morph target.\n",
    "* Xu·∫•t FBX k√®m c√°c morph target ƒë√≥.\n",
    "\n",
    "### **ML Train TOP (MLP ‚Üí ONNX)**\n",
    "\n",
    "* Input = joint rotations (feature).\n",
    "* Output = PCA coeffs.\n",
    "* Train MLP (2‚Äì3 hidden layers).\n",
    "* Xu·∫•t model `pose2latent.onnx`.\n",
    "\n",
    "### **Export Package**\n",
    "\n",
    "Ch·ª©a t·∫•t c·∫£ file UE c·∫ßn:\n",
    "\n",
    "* skeletal\\_mesh\\_with\\_morphs.fbx (mesh + morph target).\n",
    "* pose2latent.onnx (model d·ª± ƒëo√°n).\n",
    "* feature\\_stats.json (mean/std).\n",
    "* skeleton\\_map.json (mapping joint order).\n",
    "\n",
    "---\n",
    "\n",
    "# üü¶ Unreal Engine (Inference + Application)\n",
    "\n",
    "### **AnimBP ‚Äì Extract Joint Rotations**\n",
    "\n",
    "* Trong Animation Blueprint ‚Üí l·∫•y local bone rotation.\n",
    "* √Ånh x·∫° joint name theo `skeleton_map.json`.\n",
    "\n",
    "### **Normalize Features**\n",
    "\n",
    "* Chu·∫©n h√≥a theo mean/std t·ª´ Houdini (`feature_stats.json`).\n",
    "\n",
    "### **ONNX Runtime Inference (pose2latent.onnx)**\n",
    "\n",
    "* Ch·∫°y model ‚Üí output latent vector (PCA coeffs).\n",
    "\n",
    "### **Split Latent by Region**\n",
    "\n",
    "* C·∫Øt vector theo t·ª´ng region.\n",
    "* V√≠ d·ª•: 0‚Äì31 = torso, 32‚Äì63 = head, v.v.\n",
    "\n",
    "### **Apply Morph Targets (SetMorphTarget)**\n",
    "\n",
    "* G√°n coeff v√†o morph target ƒë√£ export (FBX).\n",
    "* C√≥ th·ªÉ smoothing (Exponential Filter) ƒë·ªÉ gi·∫£m jitter.\n",
    "\n",
    "### **Final Skinned Mesh with Deformation**\n",
    "\n",
    "* Nh√¢n v·∫≠t deform realtime ‚Üí kh√¥ng c·∫ßn muscle/skin sim.\n",
    "* Ready cho game/film pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "üëâ Nh∆∞ v·∫≠y pipeline n√†y:\n",
    "\n",
    "* Houdini lo ph·∫ßn **Data Prep + PCA + Training**.\n",
    "* UE lo ph·∫ßn **Inference + Runtime Deformation**.\n",
    "* ∆Øu ƒëi·ªÉm: real-time, m√¥ ph·ªèng da c∆° gi·ªëng th·∫≠t nh∆∞ng nh·∫π.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe1e5f",
   "metadata": {},
   "source": [
    "### R·∫•t t·ªët ‚Äî m√¨nh s·∫Ω ph√¢n t√≠ch **chuy√™n s√¢u, th·ª±c t·∫ø** c√°c chi·∫øn l∆∞·ª£c t·ªëi ∆∞u ho√° ƒë·ªÉ ch·∫°y inference Motion\\&Skin realtime trong **Unreal Engine** cho nhi·ªÅu nh√¢n v·∫≠t (crowd / AI characters). M√¨nh chia n·ªôi dung th√†nh: √Ω t∆∞·ªüng ch√≠nh, c√°c ph∆∞∆°ng √°n (caching, GPU inference, compute shader reconstruction, LOD & batching), chi ti·∫øt tri·ªÉn khai (ONNX ‚Üí TensorRT, l∆∞u tr·ªØ d·ªØ li·ªáu, data layout), v√≠ d·ª• pseudo-code / shader, v√† checklist ƒëo ƒë·∫øm & profiling. M√¨nh gi·ªØ m·ªçi h∆∞·ªõng d·∫´n c·ª• th·ªÉ, c√≥ th·ªÉ tri·ªÉn khai ngay.\n",
    "\n",
    "---\n",
    "\n",
    "# T√≥m t·∫Øt nhanh (tl;dr)\n",
    "\n",
    "* **Precompute & cache** latent vectors cho animation clips/poses khi c√≥ th·ªÉ ‚Üí ti·∫øt ki·ªám inference cho crowd.\n",
    "* **Ch·∫°y inference tr√™n GPU** (ONNXRuntime+CUDA ho·∫∑c TensorRT) v·ªõi FP16/INT8 ƒë·ªÉ tƒÉng throughput.\n",
    "* **Reconstruct deltas tr√™n GPU** b·∫±ng compute shader (ma tr·∫≠n PCA √ó coeffs) thay v√¨ CPU ‚Üí r·∫•t nhanh cho nhi·ªÅu instance.\n",
    "* **LOD / region split / morph-count limit**: gi·∫£m k√≠ch th∆∞·ªõc latent ho·∫∑c chuy·ªÉn sang VAT ·ªü LOD th·∫•p.\n",
    "* **Packing & batching**: pack latent v√†o textures/SSBO ƒë·ªÉ ƒë·ªçc nhi·ªÅu instance b·∫±ng 1 draw/dispatch.\n",
    "* **Quantize & prune** model ƒë·ªÉ gi·∫£m memory v√† tƒÉng perf.\n",
    "\n",
    "---\n",
    "\n",
    "# 1) Chi·∫øn l∆∞·ª£c ch√≠nh ‚Äî Khi n√†o t√≠nh tr∆∞·ªõc (cache) / khi n√†o infer realtime\n",
    "\n",
    "1. **Cache offline (n√™n ∆∞u ti√™n)**\n",
    "\n",
    "   * N·∫øu nh√¢n v·∫≠t d√πng animation clip c·ªë ƒë·ªãnh (cinematic, locomotion cycles), **precompute latent cho m·ªói frame** c·ªßa clip. L∆∞u d∆∞·ªõi d·∫°ng b·∫£ng (frame ‚Üí latent vector).\n",
    "   * Khi runtime c√≥ nhi·ªÅu instance d√πng c√πng clip, ch·ªâ c·∫ßn **index v√† ƒë·ªçc latent** theo frame, kh√¥ng ch·∫°y inference.\n",
    "   * L∆∞u ·ªü d·∫°ng texture (latent texture) ho·∫∑c SSBO cho truy c·∫≠p GPU.\n",
    "\n",
    "2. **Realtime inference (khi c·∫ßn t√≠nh pose ƒë·ªông, random)**\n",
    "\n",
    "   * D√πng ONNX/TensorRT ƒë·ªÉ infer pose‚Üílatent realtime.\n",
    "   * Ch·ªâ infer cho ‚Äúrepresentative agents‚Äù ho·∫∑c cho agents c√≥ h√†nh vi kh√°c bi·ªát; c√°c agent kh√°c tham chi·∫øu k·∫øt qu·∫£ (share) n·∫øu c√πng pose/clip.\n",
    "   * Batch nhi·ªÅu inference c√πng l√∫c ƒë·ªÉ t·∫≠n d·ª•ng GPU throughput.\n",
    "\n",
    "**Quy t·∫Øc:** n·∫øu animation/pose c√≥ th·ªÉ precompute ‚Üí h√£y precompute. Realtime inference n√™n d√†nh cho tr∆∞·ªùng h·ª£p kh√¥ng th·ªÉ d·ª± ƒëo√°n tr∆∞·ªõc (AI-driven unique poses).\n",
    "\n",
    "---\n",
    "\n",
    "# 2) GPU inference: ONNX ‚Üí TensorRT / ONNXRuntime (CUDA) ‚Äî ch·ªçn g√¨?\n",
    "\n",
    "* **ONNXRuntime (CUDA Execution Provider)**\n",
    "\n",
    "  * D·ªÖ t√≠ch h·ª£p v√†o Unreal th√¥ng qua plugin ONNXRuntime.\n",
    "  * H·ªó tr·ª£ FP32/FP16, multi-threading, c√≥ plugin Android/DirectML.\n",
    "  * T·ªët cho dev cycle nhanh.\n",
    "\n",
    "* **TensorRT** (n·∫øu target NVIDIA PC/Server)\n",
    "\n",
    "  * T·ªëi ∆∞u t·ªët h∆°n cho throughput v√† latency, h·ªó tr·ª£ FP16/INT8, layer fusion, kernel autotune.\n",
    "  * Workflow: Export ONNX ‚Üí build TRT engine (offline ho·∫∑c at startup) ‚Üí load engine in UE C++ via TensorRT runtime.\n",
    "  * **INT8** c·∫ßn calibration dataset ƒë·ªÉ gi·ªØ accuracy.\n",
    "\n",
    "**Recommendation:** Cho game desktop tr√™n NVIDIA ‚Äî **TensorRT** cho production throughput l·ªõn; cho ƒëa n·ªÅn t·∫£ng ho·∫∑c dev nhanh ‚Äî **ONNXRuntime(CUDA/DirectML)**.\n",
    "\n",
    "---\n",
    "\n",
    "# 3) S·ª≠ d·ª•ng Mixed Precision / Quantization\n",
    "\n",
    "* **FP16 (half)**: th∆∞·ªùng gi·∫£m mem √ó2 v√† tƒÉng throughput v·ªõi √≠t m·∫•t m√°t accuracy. N√™n b·∫≠t n·∫øu model h·ªó tr·ª£.\n",
    "* **INT8**: gi·∫£m h∆°n n·ªØa nh∆∞ng c·∫ßn calibration v√† ki·ªÉm tra l·ªói t√°i t·∫°o latent. D√πng cho r·∫•t large scale crowd inference.\n",
    "\n",
    "**L∆∞u √Ω**: ki·ªÉm tra MSE tƒÉng khi quantize; n·∫øu m·∫•t qu√° nhi·ªÅu detail, d√πng FP16.\n",
    "\n",
    "---\n",
    "\n",
    "# 4) T·ªëi ∆∞u batch & scheduling inference\n",
    "\n",
    "* **Batch many inputs per infer call**: inference throughput tr√™n GPU t·ªët nh·∫•t khi x·ª≠ l√Ω batch (v√≠ d·ª• batch = 32/64).\n",
    "\n",
    "* **Group agents by feature signature** (same skeleton order, same normalization): batch those together.\n",
    "\n",
    "* **Inference pipeline**:\n",
    "\n",
    "  1. Collect poses for a tick into batches (CPU)\n",
    "  2. Copy batch to GPU input buffer (staging)\n",
    "  3. Run model (ONNX/TensorRT) ‚Üí get batch of latent vectors\n",
    "  4. Store latent in GPU SSBO / texture for reconstruction pass\n",
    "\n",
    "* **Asynchronous execution**: dispatch GPU inference/compute in background GPU queues, but coordinate frame sync so results available for rendering. (Implement via RHI command lists / async compute in UE C++.)\n",
    "\n",
    "---\n",
    "\n",
    "# 5) Reconstruction on GPU ‚Äî ma tr·∫≠n PCA √ó coeffs (compute shader)\n",
    "\n",
    "Thay v√¨ t√≠nh `dP = B ¬∑ c + Œº` tr√™n CPU cho t·ª´ng vertex, ch·∫°y tr√™n GPU. L·ª£i √≠ch l·ªõn khi c√≥ nhi·ªÅu instances.\n",
    "\n",
    "## L∆∞u tr·ªØ d·ªØ li·ªáu\n",
    "\n",
    "* **Basis matrix B (V √ó K)**: l∆∞u d∆∞·ªõi d·∫°ng **texture 2D** (or structured buffer / SSBO) ‚Äî m·ªói h√†ng l√† m·ªôt vertex; m·ªói c·ªôt l√† m·ªôt component.\n",
    "* **Coefficients c (K)**: per-instance vector ‚Äî l∆∞u trong SSBO ho·∫∑c latent texture (1 texel row per instance).\n",
    "* **Mean Œº (V)**: per-vertex mean vector ‚Äî l∆∞u trong texture/SSBO.\n",
    "\n",
    "## Compute shader (pseudo HLSL)\n",
    "\n",
    "```hlsl\n",
    "// Input:\n",
    "// - RWTexture2D<float3> OutPosDelta;     // per-vertex output delta\n",
    "// - Texture2D<float> BasisTex;           // dimensions (K, V) or (V, K) depending layout\n",
    "// - StructuredBuffer<float> Coeffs;      // coefficients per instance (K) or big array for many instances\n",
    "// - Texture2D<float3> MeanTex;           // mean per vertex\n",
    "\n",
    "[numthreads(64,1,1)]\n",
    "void CSMain(uint3 dispatchThreadID : SV_DispatchThreadID)\n",
    "{\n",
    "    uint vertexIdx = dispatchThreadID.x + dispatchThreadID.y*...; // depends dispatch\n",
    "    float3 delta = float3(0,0,0);\n",
    "    // For performance: unroll small K, or read blocks\n",
    "    for (uint k=0;k<K;++k) {\n",
    "        float basis_x = BasisTex.Load(int3(k, vertexIdx, 0)); // basis value for vertexIdx, component k => scalar\n",
    "        // base stored as 3 channels if basis stores xyz per component; adjust accordingly\n",
    "        delta += basis_x * Coeffs[k];\n",
    "    }\n",
    "    delta += MeanTex.Load(int3(vertexIdx,0,0));\n",
    "    OutPosDelta[vertexIdx] = delta;\n",
    "}\n",
    "```\n",
    "\n",
    "* Dispatch once per region per instance or dispatch multi-instance with additional indexing dimension.\n",
    "* For many instances, you can run shader to handle `vertexIdx, instanceIdx` and write into per-instance vertex buffer or into GPU morph target buffer.\n",
    "\n",
    "## Data layout tips\n",
    "\n",
    "* Store `Basis` as **K textures each containing vec3 per vertex** so shader reads contiguous texel per k. Or pack K into RGBA texels to reduce fetches.\n",
    "* Use **FP16 textures** for basis & mean to reduce memory and bandwidth.\n",
    "* Coeffs for many instances can be a 2D texture: width=K, height=#instances. Sampling is cheap.\n",
    "\n",
    "---\n",
    "\n",
    "# 6) Caching latent vectors for crowd\n",
    "\n",
    "### Approach A ‚Äî Per-frame per-clip cache (best when clips known)\n",
    "\n",
    "* For each clip, precompute latent per frame:\n",
    "\n",
    "  * `latent_table[clipId][frame] = K-vector`\n",
    "* Store as 2D texture: width=K, height=framesTotal or height=instances if shared.\n",
    "* Runtime:\n",
    "\n",
    "  * For agent using clip, compute frame index ‚Üí sample latent texel row ‚Üí use for reconstruction.\n",
    "\n",
    "**Memory estimate example**\n",
    "\n",
    "* K = 128, FP16(2 bytes) per component = 256 bytes / frame.\n",
    "* Clip length = 300 frames ‚Üí \\~75 KB per clip.\n",
    "* 100 clips ‚Üí 7.5 MB. (very cheap)\n",
    "\n",
    "### Approach B ‚Äî Per-pose discretization (quantize the pose space)\n",
    "\n",
    "* Discretize pose space (hash / kmeans clustering) ‚Üí precompute latent for cluster centers.\n",
    "* At runtime, find nearest cluster center for an agent ‚Üí reuse cached latent.\n",
    "* Good for large, varied animation but with redundancy.\n",
    "\n",
    "### Approach C ‚Äî Hybrid: cache for LOD0; infer for LOD0 agents only; LOD1/2 use cheaper VAT / baked anim.\n",
    "\n",
    "* For far LODs, bake VAT or use normalized skinning only.\n",
    "\n",
    "---\n",
    "\n",
    "# 7) LOD, morph count & memory\n",
    "\n",
    "* **Morph target limit**: UE supports many morphs but each added morph increases CPU cost when set via Blueprints (depends). GPU morphing via vertex shader is preferable for hundreds of morph targets.\n",
    "* **Region split**: split body into regions with own K (torso K=128, arms K=64...). This keeps K per region manageable.\n",
    "* **LOD mapping**:\n",
    "\n",
    "  * LOD0: full PCA or AE decode via compute shader.\n",
    "  * LOD1: fewer PCA components or fewer morph targets.\n",
    "  * LOD2+: VAT or bake vertex animations.\n",
    "\n",
    "---\n",
    "\n",
    "# 8) Packing & transferring data efficiently to GPU\n",
    "\n",
    "* **Pack coeffs into 2D texture**: sample with texelFetch to avoid filtering.\n",
    "* **Use SSBO (StructuredBuffer) if on PC**: more flexible and faster for random access.\n",
    "* **Avoid per-vertex CPU uploads**: all reconstruction should happen on GPU, CPU only submits parameters (coeff index / frame index).\n",
    "* **Use persistent mapped buffers** for updates per frame to minimize copies.\n",
    "\n",
    "---\n",
    "\n",
    "# 9) Integrating into UE: practical pipeline\n",
    "\n",
    "1. **Offline**\n",
    "\n",
    "   * Export pose2latent.onnx (FP16 or INT8 with calibration).\n",
    "   * Precompute & export PCA basis textures, mean textures (FP16).\n",
    "   * Bake per-clip latent textures if possible.\n",
    "\n",
    "2. **Runtime C++ plugin**\n",
    "\n",
    "   * Initialize ONNX/TensorRT engine.\n",
    "   * For each tick:\n",
    "\n",
    "     * Gather batch of poses ‚Üí run inference (or sample cache).\n",
    "     * Upload batch latents into GPU buffer/texture.\n",
    "     * Dispatch compute shader to reconstruct per-instance vertex deltas.\n",
    "     * Use a custom vertex factory or material to read deltas and add to skinned positions.\n",
    "\n",
    "3. **Rendering**\n",
    "\n",
    "   * Vertex shader: skinning + add `delta` from reconstructed buffer or use morph target system (if morph route).\n",
    "   * Ensure synchronization (Render thread vs RHI command list).\n",
    "\n",
    "---\n",
    "\n",
    "# 10) Example pseudo-workflow (UE C++ sketch)\n",
    "\n",
    "```cpp\n",
    "// 1) Collect poses for batch, fill input tensor (batchSize, featureDim)\n",
    "// 2) if (useCache) sample latentTex[clipId][frameIdx] -> GPUUploadCoeffs(batchCoeffs)\n",
    "//    else run ONNX/TensorRT inference -> GPUUploadCoeffs(batchCoeffs)\n",
    "// 3) Dispatch compute shader:\n",
    "//    - binds BasisSSBO, MeanTex, CoeffTex, OutDeltaBuffer\n",
    "//    - Dispatch size = ceil(V / THREADS)\n",
    "// 4) In vertex shader: fetch OutDeltaBuffer[vertexIdx + instanceOffset] -> add to skinned position\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 11) Profiling & metrics ‚Äî c√°i g√¨ c·∫ßn ƒëo\n",
    "\n",
    "* **Latency per frame**: inference time (ms) + reconstruction compute (ms).\n",
    "* **Throughput**: agents processed / ms (or fps for N agents).\n",
    "* **Memory**: basis textures size, latent textures, ONNX engine memory.\n",
    "* **Bandwidth**: texture fetches in compute shader (bytes read per frame).\n",
    "* **Accuracy**: vertex L2 error vs Houdini ground-truth; temporal jitter metric.\n",
    "\n",
    "---\n",
    "\n",
    "# 12) Checklist tri·ªÉn khai (b∆∞·ªõc-b∆∞·ªõc)\n",
    "\n",
    "1. **Prototype**: CPU-only ONNX runtime ‚Üí PCA reconstruct on CPU ‚Üí verify quality.\n",
    "2. **Export ONNX FP32 ‚Üí test in UE ONNX plugin**.\n",
    "3. **Measure perf**; enable FP16 conversion ‚Üí retest accuracy.\n",
    "4. **Implement GPU reconstruction** (compute shader) reading coeffs from texture.\n",
    "5. **Batching**: group agents per-animation to process many coeffs per infer call.\n",
    "6. **Add cache layer**: per-clip per-frame latent textures for heavily reused clips.\n",
    "7. **Implement LOD**: fallback to VAT or baked skin for far LOD.\n",
    "8. **Optimize I/O**: use FP16 textures, pack channels, persistent mapped buffers.\n",
    "9. **If target NVIDIA large crowd**: build TensorRT engine (FP16/INT8) offline and integrate.\n",
    "10. **Profile** and adjust K components / model size / batch size.\n",
    "\n",
    "---\n",
    "\n",
    "# 13) V√≠ d·ª• con s·ªë ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng (quick back-of-the-envelope)\n",
    "\n",
    "* K = 128, FP16 ‚Üí 256 bytes / latent.\n",
    "* 1000 agents all unique /frame and no cache ‚Üí 256 KB per frame to fetch latents (not large). But inference cost >> memory; so caching many agents is beneficial.\n",
    "* Basis texture: V = 50k vertices, K=128 ‚Üí 50k√ó128√ó2 bytes ‚âà 12.5 MB (FP16) per region ‚Äî feasible on modern GPU if split per region and reused.\n",
    "\n",
    "---\n",
    "\n",
    "# 14) C√°c r·ªßi ro & l∆∞u √Ω th·ª±c t·∫ø\n",
    "\n",
    "* **Memory bandwidth**: reconstructing large meshes for many instances requires careful texture/SSBO layout to avoid bandwidth bottleneck.\n",
    "* **Synchronization**: must manage GPU fences for inference ‚Üí reconstruction ‚Üí rendering.\n",
    "* **Compatibility**: TensorRT only for NVIDIA; for other GPUs d√πng ONNXRuntime+DirectML or Metal Performance Shaders on Apple.\n",
    "* **Precision vs Quality**: aggressive quantization (INT8) c√≥ th·ªÉ degrade subtle surface detail ‚Äî test k·ªπ.\n",
    "\n",
    "---\n",
    "\n",
    "# K·∫øt lu·∫≠n & g·ª£i √Ω b·∫Øt tay l√†m ngay\n",
    "\n",
    "1. **B·∫Øt ƒë·∫ßu b·∫±ng cache-based route**: precompute latents per clip, integrate GPU reconstruct shader ‚Äî s·∫Ω cho l·ª£i √≠ch l·ªõn ngay.\n",
    "2. **TƒÉng d·∫ßn**: sau khi GPU reconstruct s·∫µn s√†ng, th√™m ONNXRuntime batching; cu·ªëi c√πng build TensorRT engine cho m√¥i tr∆∞·ªùng NVIDIA n·∫øu c·∫ßn throughput l·ªõn.\n",
    "3. **SPLIT v√πng & LOD**: chia regions, d√πng AE/PCA h·ª£p l√Ω, LOD fallback cho crowd.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
