{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72c231a0",
   "metadata": {},
   "source": [
    "### Module 4: Transformers in AI\n",
    "Theory\n",
    "Attention mechanism (“Attention is All You Need”)\n",
    "Encoder-decoder vs decoder-only\n",
    "Why transformers scale so well\n",
    "Applications for TAs\n",
    "Auto-sequencing VFX shots (text prompt → shot pipeline)\n",
    "AI parsing motion/text logs into structured data\n",
    "Hands-on\n",
    "Visualize transformer attention maps\n",
    "Run inference on a small transformer-based text generator\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
